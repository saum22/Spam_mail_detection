{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c904e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from os import walk\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "from collections import Counter\n",
    "import multiprocessing\n",
    "import email\n",
    "\n",
    "\n",
    "import sklearn as sk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e79246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for (root,dirs,files) in os.walk('enron_dataset', topdown=True):\n",
    "    print (root)\n",
    "    print (dirs)\n",
    "    print (files)\n",
    "    print ('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathwalk = walk(r\"enron_dataset\")\n",
    "\n",
    "allHamData, allSpamData = [], []\n",
    "for root, dr, file in pathwalk:\n",
    "    if 'ham' in str(file):\n",
    "        for obj in file:\n",
    "            with open(root + '/' + obj, encoding='latin1') as ip:\n",
    "                allHamData.append(\" \".join(ip.readlines()))\n",
    "                \n",
    "    elif 'spam' in str(file):\n",
    "        for obj in file:\n",
    "            with open(root + '/' + obj, encoding='latin1') as ip:\n",
    "                allSpamData.append(\" \".join(ip.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980da9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hamPlusSpamData = allHamData + allSpamData\n",
    "labels = [\"ham\"]*len(allHamData) + [\"spam\"]*len(allSpamData)\n",
    "\n",
    "raw_df = pd.DataFrame({\"email\": hamPlusSpamData, \n",
    "                       \"label\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b7f463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# Function to remove punctuations.\n",
    "def remove_punc(text):\n",
    "    nonP_text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return nonP_text\n",
    "\n",
    "data[\"body_text_clean\"] = data[\"email\"].apply(lambda x: remove_punc(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de595d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#function to apply tokenization\n",
    "def tokenize(text):\n",
    "    tokens = re.split(\"\\W+\", text)# W+ means all capital, small alphabets and integers 0-9\n",
    "    return tokens\n",
    "\n",
    "data[\"body_text_tokenized\"] = data[\"body_text_clean\"].apply(lambda x: tokenize(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a14d91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "def remove_stopwords(token):\n",
    "    text = [word for word in token if word not in stopwords]# to remove all stopwords\n",
    "    return text\n",
    "\n",
    "data[\"body_text_nonstop\"] = data[\"body_text_tokenized\"].apply(lambda x: remove_stopwords(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e505ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def stemming(t_text):\n",
    "    text = [ps.stem(word) for word in t_text]\n",
    "    return text\n",
    "\n",
    "data[\"body_text_stemmed\"] = data[\"body_text_nonstop\"].apply(lambda x: stemming(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a751ea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(t_text):\n",
    "    text = [wn.lemmatize(word) for word in t_text]\n",
    "    return text\n",
    "\n",
    "data[\"body_text_lemmatized\"] = data[\"body_text_stemmed\"].apply(lambda x: lemmatizer(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebdc110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "for text in data[\"body_text_lemmatized\"]:\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(text)\n",
    "    vector = vectorizer.transform(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
